

% BACKGROUND

\section{Background} 

\subsection{Digit Analysis and Benford's Law}

\begin{frame} 
	\frametitle{Digit Analysis}

\begin{itemize} 

\item Digit analysis consists in using empirical regularities regarding the occurrence of digits in numbers to screen numerical datasets for anomalies like erroneous or fraudulent data.

%\item The idea is to model a baseline frequencies distribution representing normal behaviour and then attempt to detect deviations from it.

\item Most of the digit analysis literature is based on Benford's law.

\end{itemize}

\end{frame}

\begin{frame}
	\frametitle{Benford's Law}
	
\begin{itemize}

\item \citet{newcomb1881note} noticed that in the books of logarithmic tables the first pages were more worn out, and progressively cleaner throughout.

\item Newcomb's conjecture: "The law of probability of the occurrence of numbers is such that all \textit{mantissae}  of their logarithms are equally probable"

\item \citet{benford1938} showed that Newcomb's conjecture did fit many naturally occurring collections of numbers.

\item The first rigorous proof of the emergence of Benford's law is due to \citet{hill1995derivation}.

\end{itemize}

\end{frame}

\begin{frame}

\frametitle{Benford's Law Probabilities}

\begin{itemize}

\item $ P(D_1=d_1) = \operatorname{log}\left(1+\frac{1}{d_1}\right), d_1\in \{ 1, \dots , 9\}$

\end{itemize}

\begin{table} 

\adjustbox{max height=\dimexpr\textheight-5.5cm\relax,
           max width=\textwidth}{
           
\begin{tabular}{| c | c | c | c | c | c | c | c | c | c | c |}
\hline
$d_1$ & \textbf{1} & \textbf{2} & \textbf{3} & \textbf{4} & \textbf{5} & \textbf{6} & \textbf{7} & \textbf{8} & \textbf{9} \\ 
\hline
$P({D_1}=d_1)$ & 0.3031 & 0.1761 & 0.1249 & 0.0969 & 0.0792 & 0.0669 & 0.0580 & 0.0512 & 0.0459 \\ 
\hline

\end{tabular} 
}
\caption{Benford's law probabilities for the first digit}

\end{table} 

\begin{itemize}

\item $P(D_2=d_2) = \sum_{d_1=1}^{9} \operatorname{log} \left(1+\frac{1}{10\, d_1 + d_2} \right), d_2\in \{ 0, \dots , 9\}$

\end{itemize}

\begin{table} 

\adjustbox{max height=\dimexpr\textheight-5.5cm\relax,
           max width=\textwidth}{
           
\begin{tabular}{| c | c | c | c | c | c | c | c | c | c | c | c |}
\hline
$d_2$ &\textbf{0} & \textbf{1} & \textbf{2} & \textbf{3} & \textbf{4} & \textbf{5} & \textbf{6} & \textbf{7} & \textbf{8} & \textbf{9} \\ 
\hline
$P({D_2}=d_2)$ & 0.1197 & 0.1139 & 0.1088 & 0.1043 & 0.1003 & 0.0967 & 0.0934 & 0.0904 & 0.0876 & 0.0850 \\
\hline

\end{tabular} 
}
\caption{Benford's Law probabilities for the second digit}

\end{table} 

\end{frame}

\begin{frame}
	\frametitle{Benford's Law Probabilities}
	\centering

\includegraphics[scale=.085]{R/benford.png}

\end{frame}

\begin{frame}

\frametitle{Using Benford's Law in Digit Analysis}

\begin{itemize}

\item Digit analysis is a goodness-of-fit problem.

\item A point null hypothesis represents conformance to the expected pattern.

%\begin{itemize}
%\item $H_0: \theta_0 = \theta_{BL1}$, $\theta_{BL1} = \{log(1+ \frac{1}{j}) \}_{j=1}^9$
%\item $H_0: \theta_0 = \theta_{BL2}$, $\theta_{BL2} = \left \{\sum_{i=1}^{9} \operatorname{log} \left(1+\frac{1}{10\, i + j} \right) \right \}_{j=0}^9$
%\end{itemize}

\item Classical hypothesis testing is the predominant approach.

\item Statistical tests like the ${\chi}^{2}$-test and the $z$-test are often used to assess conformance to Benford's law.

\end{itemize}

\end{frame}

\subsection{Motivation for the Bayesian Approach}

\begin{frame}
	\frametitle{In Classical Tests of Fixed Dimension:}

\begin{itemize}

\item Any deviation from the null, no matter how tiny, can be statistically significant if the sample is large enough \citep{wasserstein2016asa}. 

\item This is due to the high power the tests attain \citep{ley1996peculiar};

\item The acceptance region shrinks with sample size \citep{ley1996peculiar};

\item Usefulness and interpretation of the p-values is drastically affected by sample size \citep{pericchiTorres2011};

\item Statistical significance overweights economic significance in large samples. 


% \item P-values and statistical significance do not measure the size of the observed deviation from the null \citep{wasserstein2016asa};

% \item Concerns have been raised about the routine use of small evidence thresholds [\citet{johnson2013}, \citet{benjamin2018}].

\end{itemize}

%\item In point null hypothesis testing \citep{bergerDelampady1987}: 
%
%\begin{itemize}
%
%\item Interpreting p-values in terms of evidence against the null can be faulty;
%
%\item Bayes factors and posterior probabilities are typically at least an order of magnitude larger.

%\begin{itemize}
%
%\item In point null hypothesis testing \citep{bergerDelampady1987}: 
%
%\begin{itemize}
%
%\item Interpreting p-values in terms of evidence against the null can be faulty;
%
%\item Bayes factors and posterior probabilities are typically at least an order of magnitude larger.
%
%\end{itemize}
%
%\end{itemize}

\end{frame}

\begin{frame}
	\frametitle{Implications to Digit Analysis}

\begin{itemize}
	
\item \citet{ley1996peculiar}, \citet{pericchiTorres2011} and \citet{fonseca2016}:
	
\begin{itemize}

\item Classical significance tests often reject conformance to Benford's law in situations where both graphical inspection and conditional measures of evidence (Bayes factors and posterior probabilities) suggest otherwise. 

\item Even lower bounds on the conditional measures over wide classes of prior distributions often provide much more support to Benford's law than the p-values seem to provide. 

\item The null hypothesis is likely to be over-rejected.

\item Risk of high false-positives rate.

\end{itemize}

\end{itemize}

\end{frame}

% CONTEXT

\section{Context}

\subsection{Bayesian Digit Analysis}

\begin{frame}
\frametitle{Multinomial $\land$ Dirichlet Model}

\begin{itemize}

\item Bayesian model selection:

\begin{itemize}

\item Likelihood: $\bm{x}|\bm{\theta} \sim \mathcal{M}_k(N,\bm{\theta})$

\item Hypotheses:  $ H_0: \bm{\theta}=\bm{\theta_0} \,\,\,\, \text{vs} \,\,\,\,  H_1: \bm{\theta} \neq \bm{\theta_0}$

\item Prior: $\pi(\bm{\theta}|H_0)={1_{\bm{\theta_0}}(\bm{\theta_0})}$ and $\bm{\theta}|H_1, \bm{\alpha} \sim \operatorname{Dir}_k(\bm{\alpha})$ 

\item Marginal density: $m_i(\bm{x})=\int_{\Theta_i} f(\bm{x}|\bm{\theta})\pi(\bm{\theta}|H_i) \, d\bm{\theta} $, \, $i=0,1$

\begin{itemize}

\item  $m_0(\bm{x})=\frac{N!}{\prod_{i=1}^{k+1}x_i!}\prod_{i=1}^{k+1}{\theta_{0i}}^{x_i}$ 
\item  $m_1(\bm{x})= \frac{N!}{\prod_{i=1}^{k+1}x_i!}\frac{B(\bm{\alpha}+\bm{x})}{B(\bm{\alpha})}$ 

\end{itemize}

\item Bayes factor: $B_{01}(\bm{x}) = \frac{m_0(\bm{x})}{m_1(\bm{x})} = \frac{\prod_{i=1}^{k+1}{(\theta_{0i}}^{x_i}) \prod_{i=1}^{k+1}[\Gamma(\alpha_i)] \Gamma [\sum_{i=1}^{k+1}(\alpha_i+x_i)]}{\Gamma(\sum_{i=1}^{k+1} \alpha_i) \prod_{i=1}^{k+1}\Gamma(\alpha_i+x_i)}$

\end{itemize}	

\item Bayesian parameter estimation:

\begin{itemize}

\item Prior: $\bm{\theta}|\bm{\alpha} \sim \operatorname{Dir}_k(\bm{\alpha})$

\item Posterior: $\bm{\theta} | \bm{\alpha}, \bm{x} \sim \operatorname{Dir}_k(\bm{\alpha} + \bm{x}) \rightarrow$ Mean, mode, credible intervals.

\end{itemize}

\end{itemize}

\end{frame}

\begin{frame} 
	\frametitle{Binomial $\land$ Beta Model}

\begin{itemize}

\item Bayesian model selection:

\begin{itemize}

\item Likelihood: $x \sim \text{Bin}(N, \theta)$

\item  Hypotheses: $ H_0: {\theta}={\theta_0} \,\,\,\, \text{vs} \,\,\,\,  H_1: {\theta} \neq \theta_0$

\item Prior: $\pi(\theta|H_0)={1_{\theta_0}(\theta)}$ and $\theta| H_1 \sim \operatorname{Beta}($a$,$b$) $

\item Marginal density: $m_i(x)= \int_0^1 f(x|\theta)\pi(\theta|H_i)d \theta, \,  i=0, 1$

\begin{itemize}

\item $m_0(x)=\binom{N}{x} {\theta_0}^{x}{(1-\theta_0)}^{N-x}$ 

\item  $m_1(x) = \binom{N}{x}\frac{B(x+a, n-x+b)}{B(a,b)}$

\end{itemize}

\item Bayes factor: $B_{01}(x) = \frac{m_0(x)}{m_1(x)}  = \frac{\theta_0(1-\theta_0)^{N-x}\,\Gamma(a)\,\Gamma(b)\,\Gamma(n+a+b)}{\Gamma(a+b)\,\Gamma(n+a-x)\,\Gamma(x+a)}$ 

\end{itemize}

\item  Bayesian parameter estimation:

\begin{itemize}

\item Prior: $\theta \sim \operatorname{Beta}(a,b) $

\item Posterior: $\theta | x \sim \operatorname{Beta}(a+x, b+N-x ) \rightarrow$ Mean, mode, credible intervals

\end{itemize}

\end{itemize}

\end{frame}


\subsection{What Can Be Improved}

\begin{frame}
	\frametitle{Prior Distribution Specification}
		
\begin{itemize}

\item \citet{bergerSelke1987} and Berger and Delampady (\cite*{bergerDelampady1987}, \cite*{delampadyBerger1990}):

\begin{itemize}

\item An adequate prior for precise null hypotheses testing should be symmetrical, uni-modal, centered on the null parameter value, and non-increasing around that same parameter value.	

\item These conditions acknowledge the central role of the null parameter value (usually representing the established theory) and avoid treating parameter values other than that as special.

\end{itemize}

\end{itemize}


\end{frame}

\begin{frame}
	\frametitle{Prior Distribution Specification}

\begin{itemize}

\item The $\operatorname{Dir}_k(\bm{\alpha})$ prior for $\bm{\theta}$: 

\begin{itemize}

\item Is only centered on $\bm{\theta_0}$ if $\bm{\alpha} = c \, \bm{\theta_0}$ for some $c>0$.

\begin{itemize}

\item $\alpha_i > 1 \, \forall i \in \{1, \dots, k+1 \} \Rightarrow$  Uni-modal. 

\item For $\bm{\theta_0} = \bm{\theta_{BL1}}$ or $\bm{\theta_0} = \bm{\theta_{BL2}}$ the smallest  $c \in \mathbb{N}$ such that $c \, \bm{\theta_{0i}}>1$ for all $i$ is  $c=22$ for BL1 and $c=12$ for BL2.  

\item $\operatorname{Var}(\theta_i)$ decreases with $c$.

\end{itemize}

\item Is only symmetric if $\alpha_i = \alpha \, \forall i \in \{1, \dots, k+1 \}$.

\begin{itemize}

\item $\alpha >1 \Rightarrow$ Uni-modal, non increasing around the mean.

\end{itemize}

\end{itemize}

\item However, for $\bm{\theta_0} = \bm{\theta_{BL1}}$ or $\bm{\theta_0} = \bm{\theta_{BL2}}$:

\begin{itemize}

\item If we assume $\bm{\alpha} = c \, \bm{\theta_0}$ then $\alpha_i \neq \alpha_j$ for $i \neq j \Rightarrow$ Asymmetric prior.
\item  If we assume $\alpha_i = \alpha \, \forall i \in \{1, \dots, k+1 \}$  then $E(\theta) = \bm{1} \left(\frac{1}{k+1} \right) \Rightarrow$ not centered on $\bm{\theta_0}$. 

\end{itemize}

\end{itemize}

\end{frame}

\begin{frame}
	\frametitle{Prior Distribution Specification}

\begin{itemize}

\item The $\operatorname{Beta}(a, b)$ prior for $\theta$:

\begin{itemize}

\item Is only centered on $\theta_0$ if $a = s \, \theta_0$ and $b = s \, (1-\theta_0)$, $s>0$.

\begin{itemize}
\item $a>1$ and $b>1 \Rightarrow$  Uni-modal.  

\item For $\bm{\theta_0} \in \bm{\theta_{BL1}}$ or $\bm{\theta_0} \in \bm{\theta_{BL2}}$ the smallest $s \in \mathbb{N}$ such that  $s \, \theta_0>1$ and $s \, (1-\theta_0)>1$ are $s=22$ and $s=12$, respectively.

\item $\operatorname{Var}(\theta)$ decreases with $s$.

\end{itemize} 

\item Is only  symmetric if $a=b$. 

\begin{itemize}
\item $a=b>1$  Uni-modal, non increasing around the mean.
\end{itemize} 

\end{itemize}

\item However, for $\theta_0 \in \bm{\theta_{BL1}}$ or $\theta_0 \in \bm{\theta_{BL2}}$:

\begin{itemize}

\item If we assume $a = s \, \theta_0$ and $b = s \, (1-\theta_0)$ then it is only possible to have $a=b$ for $\theta_0 = \frac{1}{2}$. But $\theta_0 \neq \frac{1}{2} \rightarrow$ asymmetric prior.

\item If we assume $a=b$ then $E(\theta) = \frac{1}{2} \rightarrow$ not centered on $\theta_0$.

\end{itemize}

\end{itemize}

\end{frame}

%\begin{frame}
%	\frametitle{Prior Distribution Specification}
%
%\begin{itemize}
%
%\item Problems:
%
%\begin{itemize}
% 
%\item When using Bayesian methods with conjugate prior distributions to assess conformance to Benford's law we have to give up either on the prior being symmetric or on the prior being centered on the null parameter value.
%
%\item In digit analysis, the conjugate family may not be versatile enough to allow for uni-modal priors centered on the null parameter value that are not too informative.
%
%\begin{itemize}
%
%\item The results from the empirical application in \citet{fonseca2016} suggest that the $\operatorname{Beta}\,(22\,\theta_{0i}, 22-22\,\theta_{0i})$ may be too informative for BL1 digit analysis.
%
%\end{itemize}
%
%\end{itemize}
%
%\end{itemize}
%	
%\end{frame}

\begin{frame}
	\frametitle{Prior Distribution Specification}

\begin{itemize}

\item Marginal priors: $\bm{\theta} \sim \operatorname{Dir}_k(\bm{\alpha}) \Rightarrow \theta_i \sim \operatorname{Beta}(\alpha_i, \sum_{j=1}^{k+1} \alpha_j-\alpha_i).$
	
\begin{itemize}

\item Symmetric Dirichlet:   

\begin{itemize}
\item $\alpha_i = \alpha \, \forall i \in \{1, \dots, k+1 \} \Rightarrow \theta_i \sim \operatorname{Beta}(\alpha, k \alpha)$
\item Marginal priors are asymmetric and centered on $\frac{1}{k+1}$. 
\item If $\alpha>1$, these priors are uni-modal with mode $= \frac{(\alpha-1)}{(k+1)\alpha-2}$.
\end{itemize}

\item Dirichlet centered on $\bm{\theta_0}$:

\begin{itemize}
\item $\bm{\alpha} = c \,\bm{\theta_0}, c>0 \Rightarrow \theta_i \sim \operatorname{Beta}(c \, \theta_{0i}, c - c \, \theta_{0i})$
\item Marginal prior for $\theta_i$ is asymmetric, but centered on $\theta_{0i}$.
\item Marginal prior for $\theta_i$ is uni-modal if $c \, \theta_{0i} > 1$ and $c - c \theta_{0i} > 1 $.
%\item The smallest such $c \in \mathbb{N}$ is  $c=22$ for BL1 and $c=12$ for BL2.  
%\item $\operatorname{Var}(\theta_i)$ decreases with $c$.

\end{itemize}

\end{itemize}

\end{itemize}

\end{frame}

%\begin{frame}
%	\frametitle{Prior Distribution Specification}
%
%\begin{table}
%\tiny
%\centering
%\caption{{\small Hyper-parameter variances and standard deviations for the $\theta_i \, (i=1, \dots, k)$ using a $\operatorname{Beta}\,(22\,\theta_{0i}, 22-22\,\theta_{0i})$ for BL1 and a $\operatorname{Beta}\,(12\,\theta_{0i}, 12-12\,\theta_{0i})$ for BL2.}}
%
%\begin{tabular}{|c|c|c|c|c|c|}
%
%\cline{2-5}
% \multicolumn{1}{c|}{} &  \multicolumn{2}{c}{\textbf{BL1}} & \multicolumn{2}{|c|}{\textbf{BL2}} \\
%\hline
%\multicolumn{1}{|c|}{\textbf{i}} & $\bm{\sigma^2(\theta_i)}$ & $\bm{\sigma(\theta_i)}$ &  $\bm{\sigma^{2}(\theta_{i+1})}$ & $\bm{\sigma(\theta_{i+1})}$ \\ 
%\hline
%0 & 	     -        &    	-          & 	0.00810  & 	0.09002\\
%1 & 	0.00915 &	 0.09565  &	0.00776	& 0.08811\\
%2 &	0.00631 &   0.07942	 &0.00746	& 0.08637\\
%3 &	0.00475 & 	0.06895	 &0.00719	& 0.08478\\
%4 &	0.00381 &	0.06169	 &0.00694	& 0.08332\\
%5 &	0.00317 &	0.05630	 &0.00672	& 0.08196\\
%6 &	0.00272 &	0.05211	 &0.00651	& 0.08070\\
%7 &	0.00238 &	0.04874	 &0.00632	& 0.07951\\
%8 &	0.00211 &	0.04594	 &0.00615	& 0.07840\\
%9 &	0.00190 &	0.04357	 &0.00598	& 0.07735\\
%\hline
%\end{tabular} 
%\end{table}
%
%\begin{itemize}
%
%\item Uniform prior: $\sigma^2(\theta_i) = 0.08333$ and $\sigma(\theta_i) = 0.28868$.
%
%\end{itemize}
%
%\end{frame}

\begin{frame}
	\frametitle{Multiplicity Adjustment}
	
\begin{itemize}

\item In classical significance tests:

\begin{itemize}

\item The multiple testing problem arises when multiple hypotheses are tested simultaneously.

\item Each test has a specified type I error probability.

\item The family-wise type I error probability increases with $k$, the number of hypotheses tested.

\item Multiplicity adjustments aim to control the family-wise error rate at a pre-specified level.

\item Multiplicity adjustments are a long debated topic and are widely recommended.

\item Bonferroni method: adjust p-values by multiplying by $k$.

\item Review of multiplicity adjustment methods: \citet{shaffer1995multiple}.

\end{itemize}

\end{itemize}

\end{frame}


\begin{frame}
	\frametitle{Multiplicity Adjustment}

\begin{itemize}

\item Multiple comparisons may arise in digit analysis because:

\begin{itemize}

\item It often requires the same hypothesis to be tested in multiple samples;

\item It may require several Binomial hypotheses to be tested simultaneously (one for each postulated digit frequency);

\item Binomial and Multinomial hypotheses may be conducted simultaneously.

\end{itemize}

\item \citet{westfall1997} and \citet{scott2009bayesian}:

\begin{itemize}

\item In the Bayesian setting, prior model probabilities may need to be adjusted to account for multiple hypotheses.

\item There is no need do adjust posterior probabilities provided that prior probabilities are well calibrated.

\end{itemize}

\end{itemize}	

\end{frame}

\begin{frame}
	\frametitle{Multiplicity Adjustment}
	
\begin{itemize} 

\item Testing k precise hypothesis requires the specification of prior model probabilities $\pi_{0i}, i \in \{1, \dots, k \}$.

\item \citet{bergerSelke1987} consider $\pi_0=0.5$ to be the objective choice, but note that some might argue that $\pi_0$ should be higher since $H_0$ is often the established theory.

\item \citet{westfall1997}: 

\begin{itemize}

\item Motivation for Bonferroni's adjustment: $\{H_{0i} \, \text{is true, all } i \} $ is plausible, perhaps with probability near $0.5$.

\item But if $\pi_{0i}=\frac{1}{2}, i \in \{ 1, \dots, k \}$, then $P(H_{0i} \, \text{is true, all } i) = 0.5^{k}$ may be much smaller than intended. 

\item Bayes-Bonferroni adjustement: $\pi_{0i} = 0.5^{\frac{1}{k}}$.

\begin{itemize}
\item $P(H_{0i} \, \text{is true, all } i) = 0.5 $
\item $P(H_{0i}|{\operatorname{data}})^{Bon}={\left[1+\frac{1-\pi_{0i}}{\pi_{0i}}\,B_{01}{(\operatorname{data})}^{-1}\right]}^{-1} \approx k \, P(H_{0i}|\operatorname{data})$
\end{itemize}

\end{itemize}



\end{itemize}

\end{frame}

% GOALS AND CHALLENGES

\section{Goals and Challenges}

\subsection{Goals}

\begin{frame}
	\frametitle{We want to:}
\begin{itemize}

\item Improve prior distribution specification in Bayesian digit analysis by considering prior distributions from outside  the family of conjugate priors.

\item Incorporate multiplicity adjustments in Bayesian digit analysis.

\item Build user-friendly tools for testing Binomial and Multinomial precise hypotheses using adequate prior distribution specifications and multiplicity adjustments.

\item Develop R packages that help in the implementation and dissemination of the developed methodologies.

%\item Contribute to the exploration of the conflict between classical hypothesis testing and Bayesian model selection in the context of precise null hypothesis testing. Evaluate the consequences of that conflict to digit analysis.

\end{itemize}

\end{frame}

\subsection{Challenges}

\begin{frame}
	\frametitle{Challenges}

\begin{itemize}

\item Using non-conjugate prior distributions:
\begin{itemize}

\item No closed-form expressions for Bayes factors and posterior distributions;
\item Numerical solutions using MCMC simulation;
\item Programming Gibbs samplers in R using JAGS and Stan

\end{itemize}

\item \citet{liangPauloBerger2008}: Are mixtures of conjugate priors an option?

\begin{itemize}

\item Typically provides more robust inference.

\item Integrated marginal likelihood under priors for $\bm{\alpha}$, $a$, and $b$. 

\item Hierarchical structure. 

\end{itemize}

\item \citet{bayarriberger2012}:

\begin{itemize}

\item Formalize the most general and compelling criteria for objective model selection priors;
\item Derive the prior from the essential properties.

\end{itemize}

\end{itemize}




\end{frame}
